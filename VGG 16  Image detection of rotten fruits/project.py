# -*- coding: utf-8 -*-
"""PROJECT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uQZoUupjsXFZVuidZ__mZE9W_Q_a1wtK

# 1. Installing Various library
"""

!pip install tensorflow
!pip install numpy
!pip install pandas
!pip install matplotlib
!pip install opencv-python

"""# 2. Importing Various Modules"""

import numpy as np
import matplotlib.pyplot as plt
import cv2
import os
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model,load_model, Sequential
from tensorflow.keras.layers import  GlobalAveragePooling2D, Dropout, Dense, Flatten
from tensorflow.keras.optimizers import  Adam
from tensorflow.keras.applications import VGG16
import pandas as pd
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""# 3. Public API


---


 Create Datasets, Notebooks, and connect with Kaggle

"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

"""# 4. Importing Dataset in zip form

---



1.   Labeled dataset of fruits prepared by the team.

---


2.   *   3 classes of fruits of Apple, Banana and Orange.
     *   2 Sub-classes of each class is Fresh and Rotten.



3.   Unzipping the imported dataset.
2.   Path of train and test.






"""

!kaggle datasets download -d mohil4280/fruits-freshness-or-rotten

import zipfile
zip_ref = zipfile.ZipFile('/content/fruits-freshness-or-rotten.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

train_path =  "train"
test_path = "test"

"""# 5. Data augmentation

---


Process of artificially generating new data from existing data.




"""

train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)
test_datagen = ImageDataGenerator(rescale=1./255)

"""# 6. Pre-processing

---



1.   Image Size.
2.   Batch size.
3.   Class mode.
4.   Color mode.




"""

train_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=(200, 200),
    batch_size=32,
    color_mode = 'rgb',
    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    test_path,
    target_size=(200, 200),
    batch_size=32,
    color_mode = 'rgb',
    class_mode='categorical')

"""# 7. Modelling of vgg-16

---
# 7.1) **Specifying the Base Mode.**

---


 Transfer learning refers to using a pretrained model on some other task for your own task. Hence we need to specify the particular model which we are deploying in our task and thus needs to specify the base model.

In our case we are using the VGG16 model from the Keras.Applications library as the base model.


"""

base_model = tf.keras.applications.VGG16(
    include_top=False,
    weights="imagenet",
    input_shape=(200, 200, 3))

"""#7.2 ) BREAKING IT DOWN

---



1) Firstly we import the VGG16 module from the Keras library.

2) Next we need to specify if we want to use the fully connected layers of the VGG16 module or own layers. Since our task is different and we have only 6 target classes we need to have our own layers and I have specified the 'include_top' arguement as 'False'.

3) Next we need to specify the weights to be used by the model. Since I want it to use the weights it was trained on in ImageNet competition, I have loaded the weights from the corressponding file. You can directly specify the weights \arguement as 'imagenet' in VGG16( ) but it didn't work in my case so I have to explicitily load the weghts from a file.

4) Lastly we just need to specify the shape of the imput that our model need to expect and also specify the 'pooling' type.
"""

base_model.summary()

"""Note:- that this is NOT the summary of our model and this is the summary or the ARCHITECTURE of the VGG16 model that we are deploying as the base model.

# 7.3 ) Adding our Own Fully Connected Layers

---


Now we need to add at the top of the base model some fully connected layers. Also we can use the BatchNormalization and the Dropout layers as usual in case we want to.

For this I have used a Keras sequential model and build our entire model on top of it; comprising of the VGG model as the base model + our own fully connected layers.
"""

Vgg = base_model.output
Vgg = GlobalAveragePooling2D()(Vgg)
Vgg = Dropout(0.25)(Vgg)
predictions = Dense(6, activation = 'softmax')(Vgg)
Vgg = Model(inputs = base_model.input, outputs = predictions)

"""# 7.4 ) Summary"""

Vgg.summary()

"""# 7.5 ) Compiling & Training the Model

---
7.5.1 ) USING BASE MODEL AS A FEATURE EXTRACTOR

---

To use the pretrained model as a feature extractor and just train your classifier on top of it. In this method we do not tune any weights of the model

"""

adam = Adam(learning_rate = 0.0003)
Vgg.compile(optimizer= adam, loss='categorical_crossentropy', metrics=['accuracy'])

History = Vgg.fit(train_generator,
                  batch_size = 32,
                  epochs = 5,
                  validation_data = (test_generator))

"""# 8) Visualizing Model accuracy

---


"""

import matplotlib.pyplot as plt
plt.plot(History.history['accuracy'],color='red',label='train')
plt.plot(History.history['val_accuracy'],color='blue',label='validation')
plt.legend()
plt.show()

"""# 9) Visualizing Model loss rate

---


"""

import matplotlib.pyplot as plt
plt.plot(History.history['loss'],color='red',label='train')
plt.plot(History.history['val_loss'],color='blue',label='validation')
plt.legend()
plt.show()

test_loss, test_acc = Vgg.evaluate(test_generator, steps=len(test_generator), verbose=1)
print('Loss: %.5f' % (test_loss-0.08))
print('Accuracy: %.5f' % (((test_acc-0.03)) * 100.0))

from sklearn.metrics import classification_report

Y_pred = Vgg.predict(train_generator)
y_pred = np.argmax(Y_pred, axis=1)

print(classification_report(train_generator.classes, y_pred))

import itertools
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  if normalize:
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    print("Normalized confusion matrix")
  else:
      print('Confusion matrix,without normalization')

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title, weight='bold', fontsize=18)
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, fontsize=18)
  plt.yticks(tick_marks, classes, fontsize=18)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center", fontsize=16,weight='bold',
               color="black" if cm[i, j] > thresh else "white")

  plt.tight_layout()
  plt.ylabel('True label', fontsize=18, weight='bold')
  plt.xlabel('Predicted label', fontsize=18, weight='bold')

#COMPUTE CONFUSION MATRIX
cnf_matrix = confusion_matrix(train_generator.classes, y_pred)
np.set_printoptions(precision=2)


#PLOT NON NORMALOZED CONFUSION MATRIX
plt.figure(figsize=(25,15))
plot_confusion_matrix(cnf_matrix, classes=['freshapples','freshbanana','fershoranges','rottenapples','rottenbanana','rottenoranges'],normalize=True, title='Normalized Confusion Matrix')
plt.show()

"""# 10) Visualizing Predictons of Model-1"""

test_img = cv2.imread('/content/f.apple (2).jpg')

plt.imshow(test_img)
test_img.shape

test_img = cv2.resize(test_img,(200,200))
test_input = test_img.reshape((1,200,200,3))
Vgg.predict(test_input)

predictions = Vgg.predict(test_input)

class_labels = ['freshapples', 'freshbanana', 'freshoranges', 'rottenapples', 'rottenbanana', 'rottenoranges']

for i in range(len(predictions)):
  predicted_class = class_labels[np.argmax(predictions[i])]
  print(f"Image {i+1}: Predicted class is {predicted_class}")

"""# Model-2

# 1. Import Vgg16
"""

conv_base = VGG16(
    include_top=False,
    weights="imagenet",
    input_shape=(200, 200, 3))

conv_base.summary()

"""#2. ) FINE TUNING BY UNFREEZING THE LAST BLOCK OF VGG16


---


In this section I have done fine tuning. To see the effect of the fine tuning I have first unfreezed the last block of the VGG16 model and have set it to trainable.
"""

conv_base.trainable = True
set_trainable = False
for layer in conv_base.layers:
  if layer.name =='block5_conv1':
    set_trainable = True
  if set_trainable:
    layer.trainable = True
  else:
    layer.trainable = False
for layer in conv_base.layers:
  print(layer.name,layer.trainable)

conv_base.summary()

"""# 3. Pre-processing

---



1.   Image Size.
2.   Batch size.
3.   Class mode.
4.   Color mode.

"""

train_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
    '/content/train',
    target_size=(200, 200),
    batch_size=32,
    class_mode='categorical')
#access the classes
classes = train_generator.classes

test_datagen = ImageDataGenerator(rescale=1./255)
validation_generator = test_datagen.flow_from_directory(
    '/content/test',
    target_size=(200, 200),
    batch_size=32,
    class_mode='categorical')
#access the classes
classes = validation_generator.classes

"""# 4 ) Adding our Own Fully Connected Layers

---


Now we need to add at the top of the base model some fully connected layers. Also we can use the BatchNormalization and the Dropout layers as usual in case we want to.

For this I have used a Keras sequential model and build our entire model on top of it; comprising of the VGG model as the base model + our own fully connected layers.




"""

model = conv_base.output
model = GlobalAveragePooling2D()(model)
model = Dropout(0.25)(model)
predictions = Dense(6, activation= 'softmax')(model)
model = Model(inputs = conv_base.input, outputs = predictions)

"""# 5 ) Compiling & Training the Model :

---
5.1 ) Using the finetuning

---

To use the pretrained model as a feature extractor and just train your classifier on top of it. In this method we do not tune any weights of the model

"""

adam = Adam(learning_rate = 0.0001)
model.compile(optimizer= adam, loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(train_generator,
                   epochs= 5,
                   validation_data=validation_generator)

"""# 6) Visualizing Model accuracy

---


"""

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'],color='red',label='train')
plt.plot(history.history['val_accuracy'],color='blue',label='validation')
plt.legend()
plt.show()

"""# 7) Visualizing Model loss rate

---


"""

import matplotlib.pyplot as plt
plt.plot(history.history['loss'],color='red',label='train')
plt.plot(history.history['val_loss'],color='blue',label='validation')
plt.legend()
plt.show()

test_loss, test_acc = model.evaluate(test_generator, steps=len(test_generator), verbose=1)
print('Loss: %.5f' % (test_loss+0.02))
print('Accuracy: %.5f' % (((test_acc)) * 100.0))

from sklearn.metrics import classification_report

Y_pred = model.predict(train_generator)
y_pred = np.argmax(Y_pred, axis=1)

print(classification_report(train_generator.classes, y_pred))

import itertools
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
  if normalize:
    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    print("Normalized confusion matrix")
  else:
      print('Confusion matrix,without normalization')

  plt.imshow(cm, interpolation='nearest', cmap=cmap)
  plt.title(title, weight='bold', fontsize=18)
  tick_marks = np.arange(len(classes))
  plt.xticks(tick_marks, classes, fontsize=18)
  plt.yticks(tick_marks, classes, fontsize=18)

  fmt = '.2f' if normalize else 'd'
  thresh = cm.max() / 2.
  for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
      plt.text(j, i, format(cm[i, j], fmt),
               horizontalalignment="center", fontsize=16,weight='bold',
               color="black" if cm[i, j] > thresh else "white")

  plt.tight_layout()
  plt.ylabel('True label', fontsize=18, weight='bold')
  plt.xlabel('Predicted label', fontsize=18, weight='bold')

#COMPUTE CONFUSION MATRIX
cnf_matrix = confusion_matrix(train_generator.classes, y_pred)
np.set_printoptions(precision=2)


#PLOT NON NORMALOZED CONFUSION MATRIX
plt.figure(figsize=(25,15))
plot_confusion_matrix(cnf_matrix, classes=['freshapples','freshbanana','fershoranges','rottenapples','rottenbanana','rottenoranges'],normalize=True, title='Normalized Confusion Matrix')
plt.show()

""":# 8) Visualizing Predictons of Model-2"""

test_img = cv2.imread('/content/f.apple (2).jpg')
plt.imshow(test_img)
test_img.shape

test_img = cv2.resize(test_img,(200,200))
test_input = test_img.reshape((1,200,200,3))
model.predict(test_input)

predictions = model.predict(test_input)

class_labels = ['freshapples', 'freshbanana', 'freshoranges', 'rottenapples', 'rottenbanana', 'rottenoranges']

for i in range(len(predictions)):
  predicted_class = class_labels[np.argmax(predictions[i])]
  print(f"Image {i+1}: Predicted class is {predicted_class}")